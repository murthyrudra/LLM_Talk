{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654cf9b1-4f6a-420b-a540-31c19f5a3866",
   "metadata": {},
   "source": [
    "# Word Co-Occurrence Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dd511f-4099-4072-a268-9f79b96373b5",
   "metadata": {},
   "source": [
    "## This notebook demonstrates how to obtain word co-occurrence matrix and PPMI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7febeb17-30c8-4443-8c2c-27076098c57d",
   "metadata": {},
   "source": [
    "#### Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323ecdd-50fa-4cdc-90d6-1dffa6abeabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56e3fa-03c6-48f5-ac8f-20082a9bac53",
   "metadata": {},
   "source": [
    "#### Let us import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb92d5-c0a9-41a0-a829-5159fc607f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93528ac3-6d46-4963-b744-ddfff912e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "english_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2164dd37-659d-43a4-9116-0de70d0f8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Shakespeare Macbeth\n",
    "corpus = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b52856-4ce5-42ba-9355-a1da3dbfd28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [ 'I am not sleeping, I am waking, Would you know what I am making? I am boiling warm beer with butter,  Will you be my guest for supper?', \n",
    "            'Home! home! look at the shoe! Princess! the shoe was made for you! Prince! prince! take home thy bride, For she is the true one that sits by thy side!']\n",
    "\n",
    "corpus = []\n",
    "for each_sentence in sentences:\n",
    "    corpus.append( each_sentence.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f63057d-6eda-4595-90b9-2aac97e28657",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = []\n",
    "\n",
    "for each_sentence in corpus:\n",
    "    temp_sentence = []\n",
    "    for word in word_tokenize(' '.join(each_sentence)):\n",
    "        if word.lower() not in english_stop_words and word.lower() not in punctuation:\n",
    "            temp_sentence.append( word.lower())\n",
    "    tokenized_sentence.append( ' '.join(temp_sentence) )\n",
    "    \n",
    "len(tokenized_sentence)\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226ef16a-a903-4589-aba9-d3578079fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary from corpus\n",
    "\n",
    "vocabulary = {}\n",
    "\n",
    "for each_sentence in tokenized_sentence:\n",
    "    for each_word in each_sentence.split(' '):\n",
    "        if each_word not in vocabulary:\n",
    "            vocabulary[each_word] = len(vocabulary)\n",
    "            \n",
    "print('Read {0} number of unique words'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb3687-8567-4c72-99ca-afb37c92a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain bigram stats\n",
    "\n",
    "bigram_stats = {}\n",
    "unigram_stats = {}\n",
    "context_window = 2\n",
    "count = 0\n",
    "\n",
    "# for every sentence\n",
    "for each_sentence in tokenized_sentence:\n",
    "    # for every word in the sentence\n",
    "    sentence = each_sentence.split(' ')\n",
    "    for word_index in range(len(sentence)):\n",
    "        if sentence[word_index] not in unigram_stats:\n",
    "            unigram_stats[sentence[word_index]] = 1.0\n",
    "        unigram_stats[sentence[word_index]] += 1.0\n",
    "        \n",
    "        # define a context window\n",
    "        for context_window in range( 3 - word_index, 3 + word_index):\n",
    "            if context_window < 0 or context_window >= len(sentence) or context_window == word_index:\n",
    "                continue\n",
    "            if sentence[word_index] + ' ' + sentence[context_window] not in bigram_stats:\n",
    "                bigram_stats[ sentence[word_index] + ' ' + sentence[context_window] ] = 1.0\n",
    "            bigram_stats[ sentence[word_index] + ' ' + sentence[context_window] ] += 1.0\n",
    "            count = count + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6d3c87-e6bc-42af-8405-0ff9fb1121da",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_pmi = {}\n",
    "\n",
    "for word_pair in bigram_stats:\n",
    "    word_1 = word_pair.split(' ')[0]\n",
    "    word_2 = word_pair.split(' ')[1]\n",
    "    \n",
    "    word_1_prob = (unigram_stats[word_1] * 1.0)/ (count * 1.0)\n",
    "    word_2_prob = (unigram_stats[word_2] * 1.0)/ (count * 1.0)\n",
    "\n",
    "    word_1_word_2_prob = (bigram_stats[word_pair] * 1.0)/ (count * 1.0)\n",
    "\n",
    "    bigram_pmi[ word_1 + ' ' + word_2 ] =  max(math.log( word_1_word_2_prob / (word_1_prob * word_2_prob) ), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691a84d0-e0bd-4198-b5d2-870af3dd6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_pmi = dict(sorted(bigram_pmi.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b495aa4-2bad-4bc6-8924-3777206104ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bigram_pmi:\n",
    "    print(key + '\\t' + str(bigram_pmi[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc13fb1-8cbe-43e7-b1d2-23d31d3a620f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f064d8-1920-426b-b3aa-624f3ac45d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
