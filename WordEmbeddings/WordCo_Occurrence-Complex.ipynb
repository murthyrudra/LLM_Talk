{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654cf9b1-4f6a-420b-a540-31c19f5a3866",
   "metadata": {},
   "source": [
    "# Word Co-Occurrence Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dd511f-4099-4072-a268-9f79b96373b5",
   "metadata": {},
   "source": [
    "## This notebook demonstrates how to obtain word co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7febeb17-30c8-4443-8c2c-27076098c57d",
   "metadata": {},
   "source": [
    "#### Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323ecdd-50fa-4cdc-90d6-1dffa6abeabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56e3fa-03c6-48f5-ac8f-20082a9bac53",
   "metadata": {},
   "source": [
    "#### Let us import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb92d5-c0a9-41a0-a829-5159fc607f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2164dd37-659d-43a4-9116-0de70d0f8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['The top executives were on the gravy train with their huge bonuses.', \n",
    "            'His father worked hard to build the company, but all Percy has to do is sit back and ride the gravy train.',\n",
    "            'Speaking of which, let’s take our little gravy bowl and slather this plate with curry.',\n",
    "            'Don’t add water the moisture from the meat is generally enough to make gravy.',\n",
    "            'Could be that fast train is going somewhere after all.',\n",
    "            'The military uses all sorts of games to train troops.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54d547-1325-474b-8b81-392669f2b2bb",
   "metadata": {},
   "source": [
    "## Co-Occurrence Matrix without Stop-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9831360-6b95-4d72-a56f-a3d659b382fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = []\n",
    "\n",
    "for each_sentence in sentences:\n",
    "    tokenized_sentence.append( ' '.join( [word.lower() for word in word_tokenize(each_sentence)]) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435e7700-204a-4f8d-a351-c5309dba0c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226ef16a-a903-4589-aba9-d3578079fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary from corpus\n",
    "\n",
    "vocabulary = {}\n",
    "\n",
    "for each_sentence in tokenized_sentence:\n",
    "    for each_word in each_sentence.split(' '):\n",
    "        if each_word not in vocabulary:\n",
    "            vocabulary[each_word] = len(vocabulary)\n",
    "            \n",
    "print('Read {0} number of unique words'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223c712-9ad7-4c2e-9880-51e16e6c674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a nxn matrix\n",
    "\n",
    "cooccur_matrix = np.zeros(( len(vocabulary), len(vocabulary) ))\n",
    "context_window = 3\n",
    "\n",
    "\n",
    "# for every sentence\n",
    "for each_sentence in tokenized_sentence:\n",
    "    # for every word in the sentence\n",
    "    sentence = each_sentence.split(' ')\n",
    "    for word_index in range(len(sentence)):\n",
    "        # define a context window\n",
    "        for context_window in range( 3 - word_index, 3 + word_index):\n",
    "            if context_window < 0 or context_window >= len(sentence) or context_window == word_index:\n",
    "                continue\n",
    "            cooccur_matrix[ vocabulary[sentence[word_index]]][ vocabulary[sentence[context_window]] ] += 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87101a23-8e30-4df7-b6c8-d6f61e0756a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = cooccur_matrix, columns = list(vocabulary.keys()), index = list(vocabulary.keys()))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2cd53-2e4e-4c87-9840-7291f49773bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[ ['gravy', 'train', 'executives', 'fast', 'troops', 'military'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ddfc96-0ac2-4146-a0dc-03030b669eda",
   "metadata": {},
   "source": [
    "## Co-Occurrence Matrix with Stop-Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56abd0c-89f9-4830-a335-772600746681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "english_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb644d-06eb-4ab4-a0e3-a7fda00f14d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = []\n",
    "\n",
    "for each_sentence in sentences:\n",
    "    temp_sentence = []\n",
    "    for word in word_tokenize(each_sentence):\n",
    "        if word.lower() not in english_stop_words and word.lower() not in punctuation:\n",
    "            temp_sentence.append( word.lower())\n",
    "    tokenized_sentence.append( ' '.join(temp_sentence) )\n",
    "    \n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73fd0c-4b2e-41d6-8a1b-9ead8d9622d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary from corpus\n",
    "\n",
    "vocabulary = {}\n",
    "\n",
    "for each_sentence in tokenized_sentence:\n",
    "    for each_word in each_sentence.split(' '):\n",
    "        if each_word not in vocabulary:\n",
    "            vocabulary[each_word] = len(vocabulary)\n",
    "            \n",
    "print('Read {0} number of unique words'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb3687-8567-4c72-99ca-afb37c92a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a nxn matrix\n",
    "\n",
    "cooccur_matrix = np.zeros(( len(vocabulary), len(vocabulary) ))\n",
    "context_window = 3\n",
    "\n",
    "\n",
    "# for every sentence\n",
    "for each_sentence in tokenized_sentence:\n",
    "    # for every word in the sentence\n",
    "    sentence = each_sentence.split(' ')\n",
    "    for word_index in range(len(sentence)):\n",
    "        # define a context window\n",
    "        for context_window in range( 3 - word_index, 3 + word_index):\n",
    "            if context_window < 0 or context_window >= len(sentence) or context_window == word_index:\n",
    "                continue\n",
    "            cooccur_matrix[ vocabulary[sentence[word_index]]][ vocabulary[sentence[context_window]] ] += 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691a84d0-e0bd-4198-b5d2-870af3dd6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = cooccur_matrix, columns = list(vocabulary.keys()), index = list(vocabulary.keys()))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b495aa4-2bad-4bc6-8924-3777206104ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ ['gravy', 'train', 'executives', 'fast', 'troops', 'military'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a27d290-2a6b-4100-9817-4046549d196c",
   "metadata": {},
   "source": [
    "## SVD of Word Co-Occurence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222bfb9-a7a1-4dae-8aef-ba572096abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "from numpy import diag\n",
    "from numpy import dot\n",
    "from numpy import array\n",
    "\n",
    "# define a matrix\n",
    "A = array(df)\n",
    "print('Matrix A is: \\n')\n",
    "print(A)\n",
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2310a2-819c-4750-8f2c-006b49d1fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform SVD\n",
    "\n",
    "U, s, VT = svd(A)\n",
    "\n",
    "# Top k \n",
    "k = 8\n",
    "\n",
    "U = U[:,:k]\n",
    "Sigma = Sigma[:k,:k]\n",
    "VT = VT[:k,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80c4fc-09db-4890-924b-f048b6614a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimReduced_df = pd.DataFrame(U, index=vocabulary)\n",
    "dimReduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f89797-5c86-433c-98d3-6145cfc4fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def display_pca_scatterplot(model, words=None, sample=0):\n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.01, y+0.01, word)\n",
    "    plt.show()\n",
    "        \n",
    "        \n",
    "def display_tsne_scatterplot(model, words=None, sample=0):\n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "    twodim = TSNE().fit_transform(word_vectors)[:,:2]\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.01, y+0.01, word)\n",
    "    plt.savefig(\"test.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df8198-e7bd-46d2-8794-4a047c44d814",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = dimReduced_df.T.to_dict(orient='list')\n",
    "\n",
    "display_tsne_scatterplot(weights, list(dimReduced_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b7bca-04f8-491d-b5df-b9c7c79a5bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
